# 高级算法梳理

## 随机森林算法梳理

task1 8.6-8.7

#### 1、集成学习的概念

指构建多个学习器对数据集进行预测，然后用某种策略将多个学习器预测的结果集成起来，作为最终的预测结果。

#### 2、个体学习器的概念

集成学习需要先产生一组个体学习器，再用某种策略将它们结合起来。

个体学习器通常由一个现有的学习算法从训练数据产生。

“同质”集成学习中只包含同种类型的个体学习器，这里个体学习器可以称为“基学习器”。

“异质”集成学习中有不同的算法生成,个体学习器可以称为“组件学习器”。

#### 3、boosting bagging 的概念、异同点

Boosting：将弱学习器提升为强学习器的算法。先从初始训练集训练出一个基学习器，再根据基学习器的表现对训练样本分部进行调整，基于调整后的样本分布来训练下一个基学习器，如此重复直至基学习器数目达到指定的值T，最终将这T个基学习器进行加权结合。

Bagging:通过自主采样法得到T个含m个训练样本的采样集，然后基于每个采样集训练出一个基学习器，共得到T个模型，再将这些基学习器进行结合。

相同点：都为集成学习方法

不同点：

偏差-方差角度：

Boosting方法在分类任务中是通过逐步聚焦于基分类器分错的样本，减少集成分类器的偏差。Bagging方法通过对训练样本的多次采样，并分别训练出多个不同模型，然后做综合，来减小集成分类器的方差。

个体学习器的生成方式：

Boosting:个体学习器间存在强依赖关系，必须串行生成的序列化方法；Bagging:个体学习器不存在强依赖关系，可同时生成的并行化方法。

#### 4、理解不同的结合策略(平均法，投票法，学习法)

平均法：对数值型输出任务，有简单平均法，加权平均法。

一般而言，在个体学习器性能相差较大时宜使用加权平均法，而在个体学习器性能相近时宜使用简单平均法。

投票法：对分类任务，有绝对多数投票法，相对多数投票法，加权投票法。

学习法：通过另一个学习器将弱学习器的结果来进行结合，典型代表：Stacking。

#### 5、随机森林的思想

用随机的方式建立一个森林，森林里有很多的决策树组成，随机森林的每一棵决策树之间是没有关联的。每一棵决策树就是一个精通于某一个窄领域的专家（因为我们从M个特征中选择m个让每棵决策树进行学习），这样在随机森林中就有了很多个精通不同领域的专家，对一个新的问题（新的输入数据），可以用不同的角度去看待它，最终由各个专家投票得到结果。

#### 6、随机森林的推广

随机森林是Bagging的一个扩展变体。RF在以决策树为基学习器构建Bagging集成的基础上，进一步在决策树的训练过程中引入了随机属性选择。具体来说，传统决策树在选择划分属性时是在当前结点的属性集合（假定有d个属性）中选择一个最优属性；而在RF中，对基决策树的每个结点，先从该结点的属性集合中随机选择一个包含k个属性的子集，然后再从这个子集中选择一个最优属性用于划分。这也是随机森林的唯一参数。

#### 7、随机森林的优缺点

优点：

-  能够处理很高维度（feature很多）的数据，并且不用做特征选择

- 在创建随机森林的时候，对generlization error使用的是无偏估计

- 实现简单

- 训练速度快

- 容易做成并行化方法

- 在训练过程中，能够检测到feature间的互相影响

缺点：

- 随机森林在某些噪音较大的分类或回归问题上会过拟合
- 对于有不同取值的属性的数据，取值划分较多的属性会对随机森林产生更大的影响，所以随机森林在这种数据上产出的属性权值是不可信的。

#### 8、随机森林在sklearn中的参数解释

RF框架参数：

- **n_estimators**: 也就是弱学习器的最大迭代次数，或者说最大的弱学习器的个数。一般来说n_estimators太小，容易欠拟合，n_estimators太大，计算量会太大，并且n_estimators到一定的数量后，再增大n_estimators获得的模型提升会很小，所以一般选择一个适中的数值。默认是100。

- **oob_score** :即是否采用袋外样本来评估模型的好坏。默认识False。个人推荐设置为True，因为袋外分数反应了一个模型拟合后的泛化能力。

- **criterion**: 即CART树做划分时对特征的评价标准。分类模型和回归模型的损失函数是不一样的。分类RF对应的CART分类树默认是基尼系数gini,另一个可选择的标准是信息增益。回归RF对应的CART回归树默认是均方差mse，另一个可以选择的标准是绝对值差mae。一般来说选择默认的标准就已经很好的。

RF决策树参数：

-  RF划分时考虑的最大特征数**max_features**: 可以使用很多种类型的值，默认是"auto",意味着划分时最多考虑N−−√N个特征；如果是"log2"意味着划分时最多考虑log2Nlog2N个特征；如果是"sqrt"或者"auto"意味着划分时最多考虑N−−√N个特征。如果是整数，代表考虑的特征绝对数。如果是浮点数，代表考虑特征百分比，即考虑（百分比xN）取整后的特征数。其中N为样本总特征数。一般我们用默认的"auto"就可以了，如果特征数非常多，我们可以灵活使用刚才描述的其他取值来控制划分时考虑的最大特征数，以控制决策树的生成时间。

-  决策树最大深度**max_depth**: 默认可以不输入，如果不输入的话，决策树在建立子树的时候不会限制子树的深度。一般来说，数据少或者特征少的时候可以不管这个值。如果模型样本量多，特征也多的情况下，推荐限制这个最大深度，具体的取值取决于数据的分布。常用的可以取值10-100之间。

- 内部节点再划分所需最小样本数**min_samples_split**: 这个值限制了子树继续划分的条件，如果某节点的样本数少于min_samples_split，则不会继续再尝试选择最优特征来进行划分。 默认是2.如果样本量不大，不需要管这个值。如果样本量数量级非常大，则推荐增大这个值。

- 叶子节点最少样本数**min_samples_leaf**: 这个值限制了叶子节点最少的样本数，如果某叶子节点数目小于样本数，则会和兄弟节点一起被剪枝。 默认是1,可以输入最少的样本数的整数，或者最少样本数占样本总数的百分比。如果样本量不大，不需要管这个值。如果样本量数量级非常大，则推荐增大这个值。

- 叶子节点最小的样本权重和**min_weight_fraction_leaf**：这个值限制了叶子节点所有样本权重和的最小值，如果小于这个值，则会和兄弟节点一起被剪枝。 默认是0，就是不考虑权重问题。一般来说，如果我们有较多样本有缺失值，或者分类树样本的分布类别偏差很大，就会引入样本权重，这时我们就要注意这个值了。

-  最大叶子节点数**max_leaf_nodes**: 通过限制最大叶子节点数，可以防止过拟合，默认是"None”，即不限制最大的叶子节点数。如果加了限制，算法会建立在最大叶子节点数内最优的决策树。如果特征不多，可以不考虑这个值，但是如果特征分成多的话，可以加以限制，具体的值可以通过交叉验证得到。

-  节点划分最小不纯度**min_impurity_split:**  这个值限制了决策树的增长，如果某节点的不纯度(基于基尼系数，均方差)小于这个阈值，则该节点不再生成子节点。即为叶子节点 。一般不推荐改动默认值1e-7。

  上面决策树参数中最重要的包括最大特征数max_features， 最大深度max_depth， 内部节点再划分所需最小样本数min_samples_split和叶子节点最少样本数min_samples_leaf。

#### 9、随机森林的应用场景

数据维度相对低（几十维），同时对准确性有较高要求时。

因为不需要很多参数调整就可以达到不错的效果，基本上不知道用什么方法的时候都可以先试一下随机森林。

