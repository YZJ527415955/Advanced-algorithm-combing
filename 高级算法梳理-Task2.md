# 高级算法梳理-Task2

## GBDT算法梳理

#### 1、前向分步算法

前向分步算法是用来解决加法模型的优化问题。

输入:训练数据集$T={(x_1,y_1),(x_2,y_2),...(x_N,y_N)}$;损失函数L(y,f(x));基函数集${b(x;\gamma)}$;

输出：加法模型f(x)

(1)初始化$f_0(x)=0$

(2)对m=1,2，···，M

- 极小化损失函数：$(\beta_m,\gamma_m)=arg min_{\beta,\gamma}\sum{L(y_i,f_{m-1}(x_i)+\beta b(x_i;\gamma))}$得到参数$\beta_m,\gamma_m$
- 更新 $f_m(x)=f_{m-1}(x)+\beta_m b(x;\gamma_m)$

(3)得到加法模型：$f(x)=f_M(x)=\sum_{m=1}^{M}\beta_m b(x;\gamma_m)$

**前向分步算法将同时求解从m=1到M所有参数$\beta_m$,$\gamma_m$的优化问题简化为逐次求解各个$\beta_m$, $\gamma_m$ 的优化问题。**

#### 2、负梯度拟合

提升树利用加法模型与前向分步算法实现学习的优化过程。当损失函数是平方损失和指数损失函数时，每一步的优化是很简单的。但对于一般损失函数而言，往往每一步优化并不那么容易。针对这个问题，Freidman提出了梯度提升算法，这是利用最速下降法的近似方法，其关键是利用损失函数的负梯度在当前模型的值$-[\frac{\partial L(y,f(x_i))}{\partial f(x_i)}]_{f(x)=f_{m-1}(x)}$作为回归问题提升树算法中的残差的近似值，拟合一个回归树。

#### 3、损失函数

![1565260418372](D:\workspace\Markdown文档\高级算法梳理\高级算法梳理-Task2.assets\1565260418372.png)

#### 4、回归

对于回归问题，定义好损失函数后吗，GB不需要作出什么修改，计算出来的结果就是预测值。

**平方损失：**

![1565261896375](D:\workspace\Markdown文档\高级算法梳理\高级算法梳理-Task2.assets\1565261896375.png)

梯度为：![1565262164610](D:\workspace\Markdown文档\高级算法梳理\高级算法梳理-Task2.assets\1565262164610.png)

ps:对较大的偏差有很强的惩罚，并相对忽略较小的偏差。

**绝对值损失的定义，及其梯度：**

![1565262214622](D:\workspace\Markdown文档\高级算法梳理\高级算法梳理-Task2.assets\1565262214622.png)

ps:使用绝对值损失，一般比平方损失更加稳健。

**Huber损失：**

![1565262479108](D:\workspace\Markdown文档\高级算法梳理\高级算法梳理-Task2.assets\1565262479108.png)

![1565263145612](D:\workspace\Markdown文档\高级算法梳理\高级算法梳理-Task2.assets\1565263145612.png)

从图和公式可以看出，它融合了平方损失和绝对值损失。当偏差较小时，采用平方差损失；当偏差较大时，采用绝对值损失；而参数$\delta$就是用于控制偏差的临界值的。

按照作者的说法，对于正态分布的数据，Huber损失的效果近似于平方差损失，而对于长尾数据，Huber损失的效果近似于绝对值损失，而对于中等程度拖尾的数据，Huber损失的效果要优于以上两者。

#### 5、二分类、多分类

二分类和多分类采用对数损失函数(Log-Likehood Loss)，这种损失函数的目的是最大化预测值为真实值的概率。

![1565326742015](D:\workspace\Markdown文档\高级算法梳理\高级算法梳理-Task2.assets\1565326742015.png)

![1565326775349](D:\workspace\Markdown文档\高级算法梳理\高级算法梳理-Task2.assets\1565326775349.png)

![1565326958580](D:\workspace\Markdown文档\高级算法梳理\高级算法梳理-Task2.assets\1565326958580.png)



#### 6、正则化

两个方面入手：弱算法的个数M，以及收缩率v。

![1565327138823](D:\workspace\Markdown文档\高级算法梳理\高级算法梳理-Task2.assets\1565327138823.png)

![1565327181117](D:\workspace\Markdown文档\高级算法梳理\高级算法梳理-Task2.assets\1565327181117.png)

#### 7、优缺点

优点:

- 预测精度高
- 适合低维数据
- 能处理非线性数据
- 可以灵活处理各种类型的数据，包括连续值和离散值
- 在相对少的调参时间下，预测的准确率也可以比较高。
- 使用一些健壮的损失函数，对异常值的鲁棒性非常强。比如Huber损失函数和Quantile损失函数。
缺点：
- 由于弱学习器之间存在依赖关系，难以并行训练数据。

#### 8、sklearn参数

参看以下[链接](https://www.cnblogs.com/pinard/p/6143927.html)

#### 9、应用场景

GBDT可以适用于回归问题和分类问题。

**参考：**

1、《统计学习方法》

2、https://zhuanlan.zhihu.com/p/25257856

3、https://www.cnblogs.com/pinard/p/6143927.html